{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a74495ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Colab GPU available:\", torch.cuda.is_available())\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8c503bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 29 03:30:29 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   40C    P8              9W /   70W |     128MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8c5c53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t\t       profiles\t\t\t     test_colab_server.ipynb\n",
      "CMakeLists.txt\t       profiles_20251129_031351.zip  vector_add\n",
      "include\t\t       README.md\n",
      "matrix_multiplication  reduction\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "40c22393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmake version 3.31.10\n",
      "\n",
      "CMake suite maintained and supported by Kitware (kitware.com/cmake).\n"
     ]
    }
   ],
   "source": [
    "!cmake --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ed1ef",
   "metadata": {},
   "source": [
    "# Run from here always after any change in git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c5b2d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ca45e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631aa11b",
   "metadata": {},
   "source": [
    "# Make a new clone from the git repo and then run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "38e856e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Cloning into 'GPU_mode'...\n",
      "remote: Enumerating objects: 81, done.\u001b[K\n",
      "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
      "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
      "Receiving objects: 100% (81/81), 38.52 KiB | 3.21 MiB/s, done.\n",
      "remote: Total 81 (delta 40), reused 60 (delta 19), pack-reused 0 (from 0)\u001b[K\n",
      "Resolving deltas: 100% (40/40), done.\n",
      "/content/GPU_mode\n",
      "CMakeLists.txt\tmatrix_multiplication  reduction\t\tvector_add\n",
      "include\t\tREADME.md\t       test_colab_server.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Always start from /content\n",
    "%cd /content\n",
    "\n",
    "# Remove any old copies so paths don't get nested\n",
    "!rm -rf GPU_mode\n",
    "\n",
    "# Clone your repo (note the exact repo name in the URL)\n",
    "!git clone https://github.com/parthshinde1221/GPU_mode.git\n",
    "\n",
    "# Enter the repo root\n",
    "%cd GPU_mode\n",
    "\n",
    "# Sanity check: you MUST see CMakeLists.txt here\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f875e",
   "metadata": {},
   "source": [
    "# Building all CUDA kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cfcc1b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Detecting CUDA compiler ABI info\n",
      "-- Detecting CUDA compiler ABI info - done\n",
      "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
      "-- Detecting CUDA compile features\n",
      "-- Detecting CUDA compile features - done\n",
      "-- Adding matmul target: matmul_naive from /content/GPU_mode/matrix_multiplication/matmul_naive.cu\n",
      "-- Adding matmul target: matmul_tiled from /content/GPU_mode/matrix_multiplication/matmul_tiled.cu\n",
      "-- Adding vec_add target: vec_add_naive from /content/GPU_mode/vector_add/vec_add_naive.cu\n",
      "-- Adding vec_add target: vec_add_opt from /content/GPU_mode/vector_add/vec_add_opt.cu\n",
      "-- Adding reduction target: reduce_naive_add from /content/GPU_mode/reduction/reduce_naive_add.cu\n",
      "-- Adding reduction target: reduce_optimized_add from /content/GPU_mode/reduction/reduce_optimized_add.cu\n",
      "-- Configuring done (2.5s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /content/GPU_mode/build\n",
      "[  5%] \u001b[32mBuilding CUDA object matrix_multiplication/CMakeFiles/matmul_naive.dir/matmul_naive.cu.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CUDA object vector_add/CMakeFiles/vec_add_naive.dir/vec_add_naive.cu.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CUDA object matrix_multiplication/CMakeFiles/matmul_tiled.dir/matmul_tiled.cu.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CUDA object vector_add/CMakeFiles/vec_add_opt.dir/vec_add_opt.cu.o\u001b[0m\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/matrix_multiplication/matmul_naive.cu(10)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"sum\"\u001b[0m was declared but never referenced\n",
      "          float sum = 0.0f;\n",
      "                ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/matrix_multiplication/matmul_naive.cu(10)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"sum\"\u001b[0m was declared but never referenced\n",
      "          float sum = 0.0f;\n",
      "                ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/matrix_multiplication/matmul_naive.cu(10)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"sum\"\u001b[0m was declared but never referenced\n",
      "          float sum = 0.0f;\n",
      "                ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/matrix_multiplication/matmul_naive.cu(10)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"sum\"\u001b[0m was declared but never referenced\n",
      "          float sum = 0.0f;\n",
      "                ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "[ 27%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/matmul_tiled.dir/cmake_device_link.o\u001b[0m\n",
      "[ 33%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/vec_add_naive.dir/cmake_device_link.o\u001b[0m\n",
      "[ 38%] \u001b[32m\u001b[1mLinking CUDA executable ../bin/vec_add_naive\u001b[0m\n",
      "[ 44%] \u001b[32m\u001b[1mLinking CUDA executable ../bin/matmul_tiled\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/vec_add_opt.dir/cmake_device_link.o\u001b[0m\n",
      "[ 55%] \u001b[32m\u001b[1mLinking CUDA executable ../bin/vec_add_opt\u001b[0m\n",
      "[ 55%] Built target vec_add_naive\n",
      "[ 55%] Built target matmul_tiled\n",
      "[ 61%] \u001b[32mBuilding CUDA object reduction/CMakeFiles/reduce_naive_add.dir/reduce_naive_add.cu.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CUDA object reduction/CMakeFiles/reduce_optimized_add.dir/reduce_optimized_add.cu.o\u001b[0m\n",
      "[ 72%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/matmul_naive.dir/cmake_device_link.o\u001b[0m\n",
      "[ 72%] Built target vec_add_opt\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CUDA executable ../bin/matmul_naive\u001b[0m\n",
      "[ 77%] Built target matmul_naive\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(131)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"rel_eps\"\u001b[0m was declared but never referenced\n",
      "      float rel_eps = 1e-5f;\n",
      "            ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(132)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"abs_eps\"\u001b[0m was declared but never referenced\n",
      "      float abs_eps = 1e-6f;\n",
      "            ^\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(131)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"rel_eps\"\u001b[0m was declared but never referenced\n",
      "      float rel_eps = 1e-5f;\n",
      "            ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(132)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"abs_eps\"\u001b[0m was declared but never referenced\n",
      "      float abs_eps = 1e-6f;\n",
      "            ^\n",
      "\n",
      "[ 83%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/reduce_optimized_add.dir/cmake_device_link.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CUDA executable ../bin/reduce_optimized_add\u001b[0m\n",
      "/usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/11/../../../x86_64-linux-gnu/Scrt1.o: in function `_start':\n",
      "(.text+0x1b): undefined reference to `main'\n",
      "collect2: error: ld returned 1 exit status\n",
      "gmake[2]: *** [reduction/CMakeFiles/reduce_optimized_add.dir/build.make:123: bin/reduce_optimized_add] Error 1\n",
      "gmake[1]: *** [CMakeFiles/Makefile2:321: reduction/CMakeFiles/reduce_optimized_add.dir/all] Error 2\n",
      "gmake[1]: *** Waiting for unfinished jobs....\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(131)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"rel_eps\"\u001b[0m was declared but never referenced\n",
      "      float rel_eps = 1e-5f;\n",
      "            ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(132)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"abs_eps\"\u001b[0m was declared but never referenced\n",
      "      float abs_eps = 1e-6f;\n",
      "            ^\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(131)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"rel_eps\"\u001b[0m was declared but never referenced\n",
      "      float rel_eps = 1e-5f;\n",
      "            ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(132)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"abs_eps\"\u001b[0m was declared but never referenced\n",
      "      float abs_eps = 1e-6f;\n",
      "            ^\n",
      "\n",
      "[ 94%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/reduce_naive_add.dir/cmake_device_link.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CUDA executable ../bin/reduce_naive_add\u001b[0m\n",
      "[100%] Built target reduce_naive_add\n",
      "gmake: *** [Makefile:91: all] Error 2\n"
     ]
    }
   ],
   "source": [
    "# Configure the project (top-level CMakeLists.txt)\n",
    "!cmake -S . -B build -DCMAKE_BUILD_TYPE=Release\n",
    "\n",
    "# Build all targets (matmul, vec_add, etc.)\n",
    "!cmake --build build -j 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe34d054",
   "metadata": {},
   "source": [
    "# Optional build single kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0125bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernels = [\"matmul\", \"vec_add\"]  # target names from each CMakeLists.txt\n",
    "\n",
    "# for k in kernels:\n",
    "#     print(f\"\\n=== Building {k} ===\")\n",
    "#     !cmake --build build --target {k} -j 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df4574",
   "metadata": {},
   "source": [
    "# Running all Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b33cd202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global kernels\n",
    "# kernels = [\"reduce_naive_add\"]\n",
    "kernels = [\"vec_add_naive\",\"vec_add_opt\"]  # add more later: \"softmax\", \"conv\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d424f05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running vec_add_naive ===\n",
      "c[0] = 3 (expected 3)\n",
      "c[N-1] = 3 (expected 3)\n",
      "\n",
      "=== Running vec_add_opt ===\n",
      "c[0] = 3 (expected 3)\n",
      "c[N-1] = 3 (expected 3)\n"
     ]
    }
   ],
   "source": [
    "# kernels = [\"matmul_naive\", \"vec_add_naive\",\"matmul_tiled\",\"vec_add_opt\"]  # add more later: \"softmax\", \"conv\", etc.\n",
    "\n",
    "for k in kernels:\n",
    "    print(f\"\\n=== Running {k} ===\")\n",
    "    !./build/bin/{k}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b565db",
   "metadata": {},
   "source": [
    "# CUDA MemCheck all kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a30afd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2aa92b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t\tmatrix_multiplication  reduction\n",
      "CMakeLists.txt\tprofiles\t       test_colab_server.ipynb\n",
      "include\t\tREADME.md\t       vector_add\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d28f099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which cuda-memcheck\n",
    "# !which compute-sanitizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2022c2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== compute-sanitizer (memcheck) on vec_add_naive ===\n",
      "Saved profiles/vec_add_naive_memcheck.txt\n",
      "========= COMPUTE-SANITIZER\n",
      "c[0] = 3 (expected 3)\n",
      "c[N-1] = 3 (expected 3)\n",
      "========= ERROR SUMMARY: 0 errors\n",
      "\n",
      "=== compute-sanitizer (racecheck) on vec_add_naive ===\n",
      "Saved profiles/vec_add_naive_racecheck.txt\n",
      "========= COMPUTE-SANITIZER\n",
      "c[0] = 3 (expected 3)\n",
      "c[N-1] = 3 (expected 3)\n",
      "========= RACECHECK SUMMARY: 0 hazards displayed (0 errors, 0 warnings)\n",
      "\n",
      "=== compute-sanitizer (memcheck) on vec_add_opt ===\n",
      "Saved profiles/vec_add_opt_memcheck.txt\n",
      "========= COMPUTE-SANITIZER\n",
      "c[0] = 3 (expected 3)\n",
      "c[N-1] = 3 (expected 3)\n",
      "========= ERROR SUMMARY: 0 errors\n",
      "\n",
      "=== compute-sanitizer (racecheck) on vec_add_opt ===\n",
      "Saved profiles/vec_add_opt_racecheck.txt\n",
      "========= COMPUTE-SANITIZER\n",
      "c[0] = 3 (expected 3)\n",
      "c[N-1] = 3 (expected 3)\n",
      "========= RACECHECK SUMMARY: 0 hazards displayed (0 errors, 0 warnings)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"profiles\", exist_ok=True)\n",
    "\n",
    "tools   = [\"memcheck\", \"racecheck\"]\n",
    "# kernels = [\"matmul_naive\", \"vec_add_naive\", \"matmul_tiled\", \"vec_add_opt\"]\n",
    "\n",
    "for k in kernels:\n",
    "    for t in tools:\n",
    "        print(f\"\\n=== compute-sanitizer ({t}) on {k} ===\")\n",
    "        log = f\"profiles/{k}_{t}.txt\"\n",
    "        !compute-sanitizer --tool {t} ./build/bin/{k} > {log} 2>&1\n",
    "        print(f\"Saved {log}\")\n",
    "        !tail -n 20 {log}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3ad80",
   "metadata": {},
   "source": [
    "# NCU Profile each kernel all kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c77d5334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t\tmatrix_multiplication  reduction\n",
      "CMakeLists.txt\tprofiles\t       test_colab_server.ipynb\n",
      "include\t\tREADME.md\t       vector_add\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2a9463e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\t\tCMakeFiles\t     Makefile\t\t    reduction\n",
      "CMakeCache.txt\tcmake_install.cmake  matrix_multiplication  vector_add\n"
     ]
    }
   ],
   "source": [
    "!ls build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ae20b106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul_naive  matmul_tiled  reduce_naive_add  vec_add_naive  vec_add_opt\n"
     ]
    }
   ],
   "source": [
    "!ls build/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e3bcb975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import IFrame\n",
    "# import os\n",
    "\n",
    "# os.makedirs(\"profiles\", exist_ok=True)\n",
    "\n",
    "# kernels = [\"matmul\", \"vec_add\"]\n",
    "\n",
    "# for k in kernels:\n",
    "#     print(f\"\\n=== Profiling {k} with ncu ===\")\n",
    "#     # NCU_DEFAULTS=\"\" clears any default --export that Colab may set\n",
    "#     !NCU_DEFAULTS=\"\" ncu -f --set full --export html -o profiles/{k} ./build/bin/{k}\n",
    "#     display(IFrame(f\"profiles/{k}.html\", width=1024, height=600))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c12bad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running Nsight Compute and saving reports ===\n",
      "\n",
      "[1] Profiling vec_add_naive with ncu ...\n",
      "==PROF== Connected to process 18820 (/content/GPU_mode/build/bin/vec_add_naive)\n",
      "==PROF== Profiling \"vec_add_kernel\" - 0: 0%....50%....100% - 30 passes\n",
      "c[0] = 3 (expected 3)\n",
      "c[N-1] = 3 (expected 3)\n",
      "==PROF== Disconnected from process 18820\n",
      "==PROF== Report: /content/GPU_mode/profiles/vec_add_naive.ncu-rep\n",
      "--> Saved report: profiles/vec_add_naive.ncu-rep\n",
      "--> Duration for vec_add_naive:\n",
      "    Duration                         us        48.99\n",
      "\n",
      "[1] Profiling vec_add_opt with ncu ...\n",
      "==PROF== Connected to process 18913 (/content/GPU_mode/build/bin/vec_add_opt)\n",
      "==PROF== Profiling \"vec_add_kernel\" - 0: 0%....50%....100% - 30 passes\n",
      "c[0] = 3 (expected 3)\n",
      "c[N-1] = 3 (expected 3)\n",
      "==PROF== Disconnected from process 18913\n",
      "==PROF== Report: /content/GPU_mode/profiles/vec_add_opt.ncu-rep\n",
      "--> Saved report: profiles/vec_add_opt.ncu-rep\n",
      "--> Duration for vec_add_opt:\n",
      "    Duration                         us        53.50\n",
      "\n",
      "All reports saved:\n",
      "total 424K\n",
      "-rw-r--r-- 1 root root  108 Nov 29 03:30 vec_add_naive_memcheck.txt\n",
      "-rw-r--r-- 1 root root 186K Nov 29 03:31 vec_add_naive.ncu-rep\n",
      "-rw-r--r-- 1 root root  146 Nov 29 03:30 vec_add_naive_racecheck.txt\n",
      "-rw-r--r-- 1 root root  108 Nov 29 03:30 vec_add_opt_memcheck.txt\n",
      "-rw-r--r-- 1 root root 218K Nov 29 03:31 vec_add_opt.ncu-rep\n",
      "-rw-r--r-- 1 root root  146 Nov 29 03:30 vec_add_opt_racecheck.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"profiles\", exist_ok=True)\n",
    "\n",
    "# kernels = [\"matmul_naive\", \"matmul_tiled\",\"vec_add_naive\",\"vec_add_opt\"]\n",
    "\n",
    "# -------- First loop: run profiling & save reports --------\n",
    "print(\"=== Running Nsight Compute and saving reports ===\")\n",
    "for k in kernels:\n",
    "    print(f\"\\n[1] Profiling {k} with ncu ...\")\n",
    "    !NCU_DEFAULTS=\"\" ncu -f --set full -o profiles/{k} ./build/bin/{k}\n",
    "    print(f\"--> Saved report: profiles/{k}.ncu-rep\")\n",
    "    \n",
    "    # Print the kernel Duration from the report\n",
    "    print(f\"--> Duration for {k}:\")\n",
    "    !NCU_DEFAULTS=\"\" ncu --import profiles/{k}.ncu-rep --page details | grep \"Duration\"\n",
    "\n",
    "print(\"\\nAll reports saved:\")\n",
    "!ls -lh profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "65407218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Printing Nsight Compute details for each kernel ===\n",
      "\n",
      "[2] Nsight Compute details page for vec_add_naive\n",
      "[18820] vec_add_naive@127.0.0.1\n",
      "  vec_add_kernel(const float *, const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
      "    Section: GPU Speed Of Light Throughput\n",
      "    ----------------------- ----------- ------------\n",
      "    Metric Name             Metric Unit Metric Value\n",
      "    ----------------------- ----------- ------------\n",
      "    DRAM Frequency                  Ghz         4.63\n",
      "    SM Frequency                    Mhz       584.18\n",
      "    Elapsed Cycles                cycle       28,628\n",
      "    Memory Throughput                 %        89.34\n",
      "    DRAM Throughput                   %        89.34\n",
      "    Duration                         us        48.99\n",
      "    L1/TEX Cache Throughput           %        32.28\n",
      "    L2 Cache Throughput               %        29.50\n",
      "    SM Active Cycles              cycle    25,017.58\n",
      "    Compute (SM) Throughput           %        24.68\n",
      "    ----------------------- ----------- ------------\n",
      "\n",
      "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
      "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
      "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
      "\n",
      "    Section: GPU Speed Of Light Roofline Chart\n",
      "    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved       \n",
      "          close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        \n",
      "          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  \n",
      "          on roofline analysis.                                                                                         \n",
      "\n",
      "    Section: PM Sampling\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Maximum Buffer Size             Kbyte       262.14\n",
      "    Dropped Samples                sample            0\n",
      "    Maximum Sampling Interval       cycle       20,000\n",
      "    # Pass Groups                                    1\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "    Section: Compute Workload Analysis\n",
      "    -------------------- ----------- ------------\n",
      "    Metric Name          Metric Unit Metric Value\n",
      "    -------------------- ----------- ------------\n",
      "    Executed Ipc Active   inst/cycle         0.49\n",
      "    Executed Ipc Elapsed  inst/cycle         0.46\n",
      "    Issue Slots Busy               %        12.38\n",
      "    Issued Ipc Active     inst/cycle         0.50\n",
      "    SM Busy                        %        12.38\n",
      "    -------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 91.81%                                                                                    \n",
      "          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n",
      "          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n",
      "\n",
      "    Section: Memory Workload Analysis\n",
      "    ----------------- ----------- ------------\n",
      "    Metric Name       Metric Unit Metric Value\n",
      "    ----------------- ----------- ------------\n",
      "    Memory Throughput     Gbyte/s       265.01\n",
      "    Mem Busy                    %        29.50\n",
      "    Max Bandwidth               %        89.34\n",
      "    L1/TEX Hit Rate             %            0\n",
      "    L2 Hit Rate                 %        33.59\n",
      "    Mem Pipes Busy              %        24.68\n",
      "    ----------------- ----------- ------------\n",
      "\n",
      "    Section: Memory Workload Analysis Chart\n",
      "    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n",
      "          additional metric could enable the rule to provide more guidance.                                             \n",
      "\n",
      "    Section: Scheduler Statistics\n",
      "    ---------------------------- ----------- ------------\n",
      "    Metric Name                  Metric Unit Metric Value\n",
      "    ---------------------------- ----------- ------------\n",
      "    One or More Eligible                   %        12.42\n",
      "    Issued Warp Per Scheduler                        0.12\n",
      "    No Eligible                            %        87.58\n",
      "    Active Warps Per Scheduler          warp         6.24\n",
      "    Eligible Warps Per Scheduler        warp         0.15\n",
      "    ---------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 10.66%                                                                                    \n",
      "          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n",
      "          issues an instruction every 8.0 cycles. This might leave hardware resources underutilized and may lead to     \n",
      "          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n",
      "          6.24 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible warps    \n",
      "          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n",
      "          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n",
      "          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n",
      "          State Statistics and Source Counters sections.                                                                \n",
      "\n",
      "    Section: Warp State Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Warp Cycles Per Issued Instruction             cycle        50.19\n",
      "    Warp Cycles Per Executed Instruction           cycle        50.58\n",
      "    Avg. Active Threads Per Warp                                   32\n",
      "    Avg. Not Predicated Off Threads Per Warp                    29.87\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 10.66%                                                                                          \n",
      "          On average, each warp of this kernel spends 39.2 cycles being stalled waiting for a scoreboard dependency on  \n",
      "          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     \n",
      "          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        \n",
      "          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        \n",
      "          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     \n",
      "          used data to shared memory. This stall type represents about 78.2% of the total average of 50.2 cycles        \n",
      "          between issuing two instructions.                                                                             \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n",
      "          sampling data. The Kernel Profiling Guide                                                                     \n",
      "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n",
      "          on each stall reason.                                                                                         \n",
      "\n",
      "    Section: Instruction Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Avg. Executed Instructions Per Scheduler        inst        3,072\n",
      "    Executed Instructions                           inst      491,520\n",
      "    Avg. Issued Instructions Per Scheduler          inst        3,096\n",
      "    Issued Instructions                             inst      495,360\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 4.093%                                                                                          \n",
      "          This kernel executes 0 fused and 32768 non-fused FP32 instructions. By converting pairs of non-fused          \n",
      "          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           \n",
      "          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  \n",
      "          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         \n",
      "\n",
      "    Section: Launch Statistics\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Metric Name                          Metric Unit    Metric Value\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Block Size                                                   256\n",
      "    Function Cache Configuration                     CachePreferNone\n",
      "    Grid Size                                                  4,096\n",
      "    Registers Per Thread             register/thread              16\n",
      "    Shared Memory Configuration Size           Kbyte           32.77\n",
      "    Driver Shared Memory Per Block        byte/block               0\n",
      "    Dynamic Shared Memory Per Block       byte/block               0\n",
      "    Static Shared Memory Per Block        byte/block               0\n",
      "    # SMs                                         SM              40\n",
      "    Threads                                   thread       1,048,576\n",
      "    Uses Green Context                                             0\n",
      "    Waves Per SM                                               25.60\n",
      "    -------------------------------- --------------- ---------------\n",
      "\n",
      "    Section: Occupancy\n",
      "    ------------------------------- ----------- ------------\n",
      "    Metric Name                     Metric Unit Metric Value\n",
      "    ------------------------------- ----------- ------------\n",
      "    Block Limit SM                        block           16\n",
      "    Block Limit Registers                 block           16\n",
      "    Block Limit Shared Mem                block           16\n",
      "    Block Limit Warps                     block            4\n",
      "    Theoretical Active Warps per SM        warp           32\n",
      "    Theoretical Occupancy                     %          100\n",
      "    Achieved Occupancy                        %        79.47\n",
      "    Achieved Active Warps Per SM           warp        25.43\n",
      "    ------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 10.66%                                                                                          \n",
      "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.5%) can be the     \n",
      "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
      "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
      "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
      "          optimizing occupancy.                                                                                         \n",
      "\n",
      "    Section: GPU and Memory Workload Distribution\n",
      "    -------------------------- ----------- ------------\n",
      "    Metric Name                Metric Unit Metric Value\n",
      "    -------------------------- ----------- ------------\n",
      "    Average DRAM Active Cycles       cycle   202,861.50\n",
      "    Total DRAM Elapsed Cycles        cycle    1,816,576\n",
      "    Average L1 Active Cycles         cycle    25,017.58\n",
      "    Total L1 Elapsed Cycles          cycle    1,062,384\n",
      "    Average L2 Active Cycles         cycle    35,986.19\n",
      "    Total L2 Elapsed Cycles          cycle    1,338,560\n",
      "    Average SM Active Cycles         cycle    25,017.58\n",
      "    Total SM Elapsed Cycles          cycle    1,062,384\n",
      "    Average SMSP Active Cycles       cycle    24,917.69\n",
      "    Total SMSP Elapsed Cycles        cycle    4,249,536\n",
      "    -------------------------- ----------- ------------\n",
      "\n",
      "    Section: Source Counters\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Branch Instructions Ratio           %         0.13\n",
      "    Branch Instructions              inst       65,536\n",
      "    Branch Efficiency                   %            0\n",
      "    Avg. Divergent Branches                          0\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "\n",
      "[2] Nsight Compute details page for vec_add_opt\n",
      "[18913] vec_add_opt@127.0.0.1\n",
      "  vec_add_kernel(const float *, const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
      "    Section: GPU Speed Of Light Throughput\n",
      "    ----------------------- ----------- ------------\n",
      "    Metric Name             Metric Unit Metric Value\n",
      "    ----------------------- ----------- ------------\n",
      "    DRAM Frequency                  Ghz         4.75\n",
      "    SM Frequency                    Mhz       584.42\n",
      "    Elapsed Cycles                cycle       31,275\n",
      "    Memory Throughput                 %        83.92\n",
      "    DRAM Throughput                   %        83.92\n",
      "    Duration                         us        53.50\n",
      "    L1/TEX Cache Throughput           %        29.76\n",
      "    L2 Cache Throughput               %        27.01\n",
      "    SM Active Cycles              cycle    28,272.22\n",
      "    Compute (SM) Throughput           %        36.70\n",
      "    ----------------------- ----------- ------------\n",
      "\n",
      "    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   \n",
      "          further improve performance, work will likely need to be shifted from the most utilized to another unit.      \n",
      "          Start by analyzing DRAM in the Memory Workload Analysis section.                                              \n",
      "\n",
      "    Section: GPU Speed Of Light Roofline Chart\n",
      "    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved       \n",
      "          close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        \n",
      "          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  \n",
      "          on roofline analysis.                                                                                         \n",
      "\n",
      "    Section: PM Sampling\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Maximum Buffer Size             Kbyte       262.14\n",
      "    Dropped Samples                sample            0\n",
      "    Maximum Sampling Interval       cycle       20,000\n",
      "    # Pass Groups                                    1\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "    Section: Compute Workload Analysis\n",
      "    -------------------- ----------- ------------\n",
      "    Metric Name          Metric Unit Metric Value\n",
      "    -------------------- ----------- ------------\n",
      "    Executed Ipc Active   inst/cycle         1.54\n",
      "    Executed Ipc Elapsed  inst/cycle         1.46\n",
      "    Issue Slots Busy               %        38.48\n",
      "    Issued Ipc Active     inst/cycle         1.54\n",
      "    SM Busy                        %        38.48\n",
      "    -------------------- ----------- ------------\n",
      "\n",
      "    INF   FMA is the highest-utilized pipeline (27.5%) based on active cycles, taking into account the rates of its     \n",
      "          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    \n",
      "          operations. It is well-utilized, but should not be a bottleneck.                                              \n",
      "\n",
      "    Section: Memory Workload Analysis\n",
      "    ----------------- ----------- ------------\n",
      "    Metric Name       Metric Unit Metric Value\n",
      "    ----------------- ----------- ------------\n",
      "    Memory Throughput     Gbyte/s       255.17\n",
      "    Mem Busy                    %        27.01\n",
      "    Max Bandwidth               %        83.92\n",
      "    L1/TEX Hit Rate             %            0\n",
      "    L2 Hit Rate                 %        33.54\n",
      "    Mem Pipes Busy              %        22.11\n",
      "    ----------------- ----------- ------------\n",
      "\n",
      "    Section: Memory Workload Analysis Chart\n",
      "    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n",
      "          additional metric could enable the rule to provide more guidance.                                             \n",
      "\n",
      "    Section: Scheduler Statistics\n",
      "    ---------------------------- ----------- ------------\n",
      "    Metric Name                  Metric Unit Metric Value\n",
      "    ---------------------------- ----------- ------------\n",
      "    One or More Eligible                   %        38.37\n",
      "    Issued Warp Per Scheduler                        0.38\n",
      "    No Eligible                            %        61.63\n",
      "    Active Warps Per Scheduler          warp         6.35\n",
      "    Eligible Warps Per Scheduler        warp         0.58\n",
      "    ---------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 16.08%                                                                                    \n",
      "          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n",
      "          issues an instruction every 2.6 cycles. This might leave hardware resources underutilized and may lead to     \n",
      "          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n",
      "          6.35 active warps per scheduler, but only an average of 0.58 warps were eligible per cycle. Eligible warps    \n",
      "          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n",
      "          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n",
      "          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  \n",
      "          State Statistics and Source Counters sections.                                                                \n",
      "\n",
      "    Section: Warp State Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Warp Cycles Per Issued Instruction             cycle        16.55\n",
      "    Warp Cycles Per Executed Instruction           cycle        16.59\n",
      "    Avg. Active Threads Per Warp                                   32\n",
      "    Avg. Not Predicated Off Threads Per Warp                    27.77\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 16.08%                                                                                          \n",
      "          On average, each warp of this kernel spends 9.0 cycles being stalled waiting for a scoreboard dependency on a \n",
      "          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  \n",
      "          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      \n",
      "          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    \n",
      "          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   \n",
      "          shared memory. This stall type represents about 54.5% of the total average of 16.6 cycles between issuing     \n",
      "          two instructions.                                                                                             \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n",
      "          sampling data. The Kernel Profiling Guide                                                                     \n",
      "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n",
      "          on each stall reason.                                                                                         \n",
      "\n",
      "    Section: Instruction Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Avg. Executed Instructions Per Scheduler        inst    10,854.40\n",
      "    Executed Instructions                           inst    1,736,704\n",
      "    Avg. Issued Instructions Per Scheduler          inst    10,878.40\n",
      "    Issued Instructions                             inst    1,740,544\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 13.76%                                                                                          \n",
      "          This kernel executes 0 fused and 32768 non-fused FP32 instructions. By converting pairs of non-fused          \n",
      "          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           \n",
      "          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  \n",
      "          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         \n",
      "\n",
      "    Section: Launch Statistics\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Metric Name                          Metric Unit    Metric Value\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Block Size                                                   256\n",
      "    Function Cache Configuration                     CachePreferNone\n",
      "    Grid Size                                                  4,096\n",
      "    Registers Per Thread             register/thread              30\n",
      "    Shared Memory Configuration Size           Kbyte           32.77\n",
      "    Driver Shared Memory Per Block        byte/block               0\n",
      "    Dynamic Shared Memory Per Block       byte/block               0\n",
      "    Static Shared Memory Per Block        byte/block               0\n",
      "    # SMs                                         SM              40\n",
      "    Threads                                   thread       1,048,576\n",
      "    Uses Green Context                                             0\n",
      "    Waves Per SM                                               25.60\n",
      "    -------------------------------- --------------- ---------------\n",
      "\n",
      "    Section: Occupancy\n",
      "    ------------------------------- ----------- ------------\n",
      "    Metric Name                     Metric Unit Metric Value\n",
      "    ------------------------------- ----------- ------------\n",
      "    Block Limit SM                        block           16\n",
      "    Block Limit Registers                 block            8\n",
      "    Block Limit Shared Mem                block           16\n",
      "    Block Limit Warps                     block            4\n",
      "    Theoretical Active Warps per SM        warp           32\n",
      "    Theoretical Occupancy                     %          100\n",
      "    Achieved Occupancy                        %        81.63\n",
      "    Achieved Active Warps Per SM           warp        26.12\n",
      "    ------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 16.08%                                                                                          \n",
      "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (81.6%) can be the     \n",
      "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
      "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
      "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
      "          optimizing occupancy.                                                                                         \n",
      "\n",
      "    Section: GPU and Memory Workload Distribution\n",
      "    -------------------------- ----------- ------------\n",
      "    Metric Name                Metric Unit Metric Value\n",
      "    -------------------------- ----------- ------------\n",
      "    Average DRAM Active Cycles       cycle      213,320\n",
      "    Total DRAM Elapsed Cycles        cycle    2,033,664\n",
      "    Average L1 Active Cycles         cycle    28,272.22\n",
      "    Total L1 Elapsed Cycles          cycle    1,185,664\n",
      "    Average L2 Active Cycles         cycle    40,415.06\n",
      "    Total L2 Elapsed Cycles          cycle    1,462,432\n",
      "    Average SM Active Cycles         cycle    28,272.22\n",
      "    Total SM Elapsed Cycles          cycle    1,185,664\n",
      "    Average SMSP Active Cycles       cycle    28,351.45\n",
      "    Total SMSP Elapsed Cycles        cycle    4,742,656\n",
      "    -------------------------- ----------- ------------\n",
      "\n",
      "    Section: Source Counters\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Branch Instructions Ratio           %         0.09\n",
      "    Branch Instructions              inst      163,840\n",
      "    Branch Efficiency                   %          100\n",
      "    Avg. Divergent Branches                          0\n",
      "    ------------------------- ----------- ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------- Second loop: re-import reports and show details --------\n",
    "print(\"\\n=== Printing Nsight Compute details for each kernel ===\")\n",
    "for k in kernels:\n",
    "    print(f\"\\n[2] Nsight Compute details page for {k}\")\n",
    "    !NCU_DEFAULTS=\"\" ncu --import profiles/{k}.ncu-rep --page details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "130fb77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t\tmatrix_multiplication  reduction\n",
      "CMakeLists.txt\tprofiles\t       test_colab_server.ipynb\n",
      "include\t\tREADME.md\t       vector_add\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97fd7bb",
   "metadata": {},
   "source": [
    "# Create a Zip File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5a81cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Adjust this if your repo root is different\n",
    "# repo_root = \"/content/GPU_mode\"\n",
    "# profiles_dir = os.path.join(repo_root, \"profiles\")\n",
    "\n",
    "# # Make sure the profiles folder exists\n",
    "# if os.path.isdir(profiles_dir):\n",
    "#     # Create profiles.zip next to the repo root\n",
    "#     zip_path = os.path.join(repo_root, \"profiles\")\n",
    "#     shutil.make_archive(zip_path, \"zip\", profiles_dir)\n",
    "#     print(f\"Created ZIP: {zip_path}.zip\")\n",
    "# else:\n",
    "#     print(f\"No profiles folder found at: {profiles_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8c331104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ZIP: /content/GPU_mode/profiles_20251129_033106.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "repo_root = \"/content/GPU_mode\"\n",
    "profiles_dir = os.path.join(repo_root, \"profiles\")\n",
    "\n",
    "# Make timestamped name\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "zip_basename = f\"profiles_{timestamp}\"             # without .zip\n",
    "zip_path = os.path.join(repo_root, zip_basename)   # /content/GPU_mode/profiles_YYYYMMDD_HHMMSS\n",
    "\n",
    "if os.path.isdir(profiles_dir):\n",
    "    # Create /content/GPU_mode/profiles_YYYYMMDD_HHMMSS.zip\n",
    "    shutil.make_archive(zip_path, \"zip\", profiles_dir)\n",
    "    zip_file = zip_path + \".zip\"\n",
    "    print(f\"Created ZIP: {zip_file}\")\n",
    "else:\n",
    "    print(f\"No profiles folder found at: {profiles_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "18d3a14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t\t       profiles\t\t\t     test_colab_server.ipynb\n",
      "CMakeLists.txt\t       profiles_20251129_033106.zip  vector_add\n",
      "include\t\t       README.md\n",
      "matrix_multiplication  reduction\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a8dbf",
   "metadata": {},
   "source": [
    "# File Downloader Widget/Workflow Provided by Colab For the Profiler Zip File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7fa9706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anywidget\n",
    "import traitlets\n",
    "import os\n",
    "import base64\n",
    "import mimetypes\n",
    "\n",
    "class FileDownloader(anywidget.AnyWidget):\n",
    "    \"\"\"\n",
    "    An anywidget that renders a button. When clicked, it triggers a server-side\n",
    "    read of 'file_path' and sends the content to the browser for download.\n",
    "\n",
    "    The button starts disabled and only enables after a successful handshake\n",
    "    with the Python kernel, ensuring it doesn't appear active in a dead notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    # The path to the file on the server/local disk that you want to download\n",
    "    file_path = traitlets.Unicode(help=\"Path to the file to be downloaded\").tag(sync=True)\n",
    "\n",
    "    # Label for the button\n",
    "    button_text = traitlets.Unicode(\"Download File\").tag(sync=True)\n",
    "\n",
    "    _esm = \"\"\"\n",
    "    export function render({ model, el }) {\n",
    "      // Create the button element\n",
    "      let btn = document.createElement(\"button\");\n",
    "      btn.classList.add(\"jupyter-widgets\", \"jupyter-button\", \"widget-button\");\n",
    "      btn.style.width = \"100%\";\n",
    "\n",
    "      // Initial state: Disabled and waiting\n",
    "      btn.innerText = \"Waiting for Kernel...\";\n",
    "      btn.disabled = true;\n",
    "\n",
    "      // Update button text if the Python trait changes\n",
    "      model.on(\"change:button_text\", () => {\n",
    "        // Only update visually if we are already connected/enabled\n",
    "        if (!btn.disabled) {\n",
    "            btn.innerText = model.get(\"button_text\");\n",
    "        }\n",
    "      });\n",
    "\n",
    "      // Handle the click event\n",
    "      btn.addEventListener(\"click\", () => {\n",
    "        const filePath = model.get(\"file_path\");\n",
    "\n",
    "        if (!filePath) {\n",
    "            alert(\"No file path set in the Python widget!\");\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        // Disable button and show loading state\n",
    "        const originalText = btn.innerText;\n",
    "        btn.innerText = \"Downloading...\";\n",
    "        btn.disabled = true;\n",
    "\n",
    "        // Send a request message to the Python backend\n",
    "        model.send({ type: \"request_download\" });\n",
    "\n",
    "        // Helper to restore button state\n",
    "        const restoreBtn = () => {\n",
    "            btn.innerText = originalText;\n",
    "            btn.disabled = false;\n",
    "        };\n",
    "\n",
    "        // Timeout safety to restore button if Python doesn't respond within 5s\n",
    "        setTimeout(restoreBtn, 5000);\n",
    "      });\n",
    "\n",
    "      el.appendChild(btn);\n",
    "\n",
    "      // Listen for messages coming from Python\n",
    "      model.on(\"msg:custom\", (msg) => {\n",
    "        if (msg.type === \"connection_verified\") {\n",
    "            // HANDSHAKE COMPLETE: Kernel is alive.\n",
    "            btn.disabled = false;\n",
    "            btn.innerText = model.get(\"button_text\");\n",
    "        }\n",
    "        else if (msg.type === \"file_content\") {\n",
    "            // 1. Create a Blob from the Base64 data\n",
    "            const byteCharacters = atob(msg.content);\n",
    "            const byteNumbers = new Array(byteCharacters.length);\n",
    "            for (let i = 0; i < byteCharacters.length; i++) {\n",
    "                byteNumbers[i] = byteCharacters.charCodeAt(i);\n",
    "            }\n",
    "            const byteArray = new Uint8Array(byteNumbers);\n",
    "            const blob = new Blob([byteArray], { type: msg.mime_type });\n",
    "\n",
    "            // 2. Create a temporary link to trigger the download\n",
    "            const url = window.URL.createObjectURL(blob);\n",
    "            const a = document.createElement(\"a\");\n",
    "            a.style.display = \"none\";\n",
    "            a.href = url;\n",
    "            a.download = msg.filename;\n",
    "            document.body.appendChild(a);\n",
    "            a.click();\n",
    "\n",
    "            // 3. Cleanup\n",
    "            window.URL.revokeObjectURL(url);\n",
    "            document.body.removeChild(a);\n",
    "\n",
    "            // Restore button text\n",
    "            btn.innerText = model.get(\"button_text\");\n",
    "            btn.disabled = false;\n",
    "\n",
    "        } else if (msg.type === \"error\") {\n",
    "            alert(`Error: ${msg.message}`);\n",
    "            btn.innerText = model.get(\"button_text\");\n",
    "            btn.disabled = false;\n",
    "        }\n",
    "      });\n",
    "\n",
    "      // INITIATE HANDSHAKE\n",
    "      // Send a message to Python to check if the kernel is listening.\n",
    "      // If the kernel is dead (saved notebook), this message goes nowhere,\n",
    "      // and the button remains disabled.\n",
    "      setTimeout(() => {\n",
    "        model.send({ type: \"check_connection\" });\n",
    "      }, 500);\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if file_path:\n",
    "            self.file_path = file_path\n",
    "\n",
    "        # Register the message handler\n",
    "        self.on_msg(self._handle_custom_msg)\n",
    "\n",
    "    def _handle_custom_msg(self, msg, content):\n",
    "        \"\"\"\n",
    "        Callback for when the frontend sends a message to Python.\n",
    "        \"\"\"\n",
    "        msg_type = msg.get(\"type\")\n",
    "\n",
    "        if msg_type == \"check_connection\":\n",
    "            # Reply to the frontend to confirm we are alive\n",
    "            self.send({\"type\": \"connection_verified\"})\n",
    "\n",
    "        elif msg_type == \"request_download\":\n",
    "            self._process_download()\n",
    "\n",
    "    def _process_download(self):\n",
    "        \"\"\"\n",
    "        Reads the file from disk and sends it to the frontend.\n",
    "        \"\"\"\n",
    "        target_path = self.file_path\n",
    "\n",
    "        # Basic validation\n",
    "        if not target_path:\n",
    "            self.send({\"type\": \"error\", \"message\": \"File path is not defined.\"})\n",
    "            return\n",
    "\n",
    "        if not os.path.exists(target_path):\n",
    "            self.send({\"type\": \"error\", \"message\": f\"File not found: {target_path}\"})\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Guess the MIME type so the browser handles it correctly\n",
    "            mime_type, _ = mimetypes.guess_type(target_path)\n",
    "            if mime_type is None:\n",
    "                mime_type = 'application/octet-stream'\n",
    "\n",
    "            # Read and encode the file\n",
    "            with open(target_path, \"rb\") as f:\n",
    "                file_content = f.read()\n",
    "\n",
    "            b64_content = base64.b64encode(file_content).decode(\"utf-8\")\n",
    "\n",
    "            # Send back to JS\n",
    "            self.send({\n",
    "                \"type\": \"file_content\",\n",
    "                \"filename\": os.path.basename(target_path),\n",
    "                \"mime_type\": mime_type,\n",
    "                \"content\": b64_content\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            self.send({\"type\": \"error\", \"message\": str(e)})\n",
    "\n",
    "# dummy_filename = \"example_data.txt\"\n",
    "# with open(dummy_filename, \"w\") as f:\n",
    "#     f.write(\"Hello! This is a file dynamically read from the kernel disk.\\n\")\n",
    "#     f.write(\"If you are reading this, the widget worked.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9d35434b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/GPU_mode/profiles_20251129_033106.zip\n"
     ]
    }
   ],
   "source": [
    "print(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7fcb143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t\t       profiles\t\t\t     test_colab_server.ipynb\n",
      "CMakeLists.txt\t       profiles_20251129_033106.zip  vector_add\n",
      "include\t\t       README.md\n",
      "matrix_multiplication  reduction\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e81de5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the timestamped zip with FileDownloader\n",
    "# FileDownloader(file_path=\"/content/GPU_mode/test_colab_server.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e135d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import FileLink\n",
    "\n",
    "# zip_path = \"/content/GPU_mode/profiles_run_2025-11-28T19-12-00.zip\"\n",
    "# FileLink(zip_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8963a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a74495ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Colab GPU available:\", torch.cuda.is_available())\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8c503bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 29 03:13:18 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   40C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8c5c53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t\t       profiles\t\t\t     test_colab_server.ipynb\n",
      "CMakeLists.txt\t       profiles_20251129_030520.zip  vector_add\n",
      "include\t\t       README.md\n",
      "matrix_multiplication  reduction\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "40c22393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmake version 3.31.10\n",
      "\n",
      "CMake suite maintained and supported by Kitware (kitware.com/cmake).\n"
     ]
    }
   ],
   "source": [
    "!cmake --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ed1ef",
   "metadata": {},
   "source": [
    "# Run from here always after any change in git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c5b2d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ca45e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631aa11b",
   "metadata": {},
   "source": [
    "# Make a new clone from the git repo and then run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "38e856e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Cloning into 'GPU_mode'...\n",
      "remote: Enumerating objects: 71, done.\u001b[K\n",
      "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
      "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
      "Receiving objects: 100% (71/71), 34.62 KiB | 5.77 MiB/s, done.\n",
      "remote: Total 71 (delta 32), reused 54 (delta 15), pack-reused 0 (from 0)\u001b[K\n",
      "Resolving deltas: 100% (32/32), done.\n",
      "/content/GPU_mode\n",
      "CMakeLists.txt\tmatrix_multiplication  reduction\t\tvector_add\n",
      "include\t\tREADME.md\t       test_colab_server.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Always start from /content\n",
    "%cd /content\n",
    "\n",
    "# Remove any old copies so paths don't get nested\n",
    "!rm -rf GPU_mode\n",
    "\n",
    "# Clone your repo (note the exact repo name in the URL)\n",
    "!git clone https://github.com/parthshinde1221/GPU_mode.git\n",
    "\n",
    "# Enter the repo root\n",
    "%cd GPU_mode\n",
    "\n",
    "# Sanity check: you MUST see CMakeLists.txt here\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f875e",
   "metadata": {},
   "source": [
    "# Building all CUDA kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cfcc1b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Detecting CUDA compiler ABI info\n",
      "-- Detecting CUDA compiler ABI info - done\n",
      "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
      "-- Detecting CUDA compile features\n",
      "-- Detecting CUDA compile features - done\n",
      "-- Adding matmul target: matmul_naive from /content/GPU_mode/matrix_multiplication/matmul_naive.cu\n",
      "-- Adding matmul target: matmul_tiled from /content/GPU_mode/matrix_multiplication/matmul_tiled.cu\n",
      "-- Adding vec_add target: vec_add_naive from /content/GPU_mode/vector_add/vec_add_naive.cu\n",
      "-- Adding vec_add target: vec_add_opt from /content/GPU_mode/vector_add/vec_add_opt.cu\n",
      "-- Adding reduction target: reduce_naive_add from /content/GPU_mode/reduction/reduce_naive_add.cu\n",
      "-- Adding reduction target: reduce_optimized_add from /content/GPU_mode/reduction/reduce_optimized_add.cu\n",
      "-- Configuring done (2.9s)\n",
      "-- Generating done (0.0s)\n",
      "-- Build files have been written to: /content/GPU_mode/build\n",
      "[  5%] \u001b[32mBuilding CUDA object vector_add/CMakeFiles/vec_add_opt.dir/vec_add_opt.cu.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CUDA object matrix_multiplication/CMakeFiles/matmul_naive.dir/matmul_naive.cu.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CUDA object vector_add/CMakeFiles/vec_add_naive.dir/vec_add_naive.cu.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CUDA object matrix_multiplication/CMakeFiles/matmul_tiled.dir/matmul_tiled.cu.o\u001b[0m\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/matrix_multiplication/matmul_naive.cu(10)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"sum\"\u001b[0m was declared but never referenced\n",
      "          float sum = 0.0f;\n",
      "                ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/matrix_multiplication/matmul_naive.cu(10)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"sum\"\u001b[0m was declared but never referenced\n",
      "          float sum = 0.0f;\n",
      "                ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/matrix_multiplication/matmul_naive.cu(10)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"sum\"\u001b[0m was declared but never referenced\n",
      "          float sum = 0.0f;\n",
      "                ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/matrix_multiplication/matmul_naive.cu(10)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"sum\"\u001b[0m was declared but never referenced\n",
      "          float sum = 0.0f;\n",
      "                ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "[ 27%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/matmul_tiled.dir/cmake_device_link.o\u001b[0m\n",
      "[ 33%] \u001b[32m\u001b[1mLinking CUDA executable ../bin/matmul_tiled\u001b[0m\n",
      "[ 33%] Built target matmul_tiled\n",
      "[ 38%] \u001b[32mBuilding CUDA object reduction/CMakeFiles/reduce_naive_add.dir/reduce_naive_add.cu.o\u001b[0m\n",
      "[ 44%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/matmul_naive.dir/cmake_device_link.o\u001b[0m\n",
      "[ 50%] \u001b[32m\u001b[1mLinking CUDA executable ../bin/matmul_naive\u001b[0m\n",
      "[ 55%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/vec_add_opt.dir/cmake_device_link.o\u001b[0m\n",
      "[ 55%] Built target matmul_naive\n",
      "[ 61%] \u001b[32mBuilding CUDA object reduction/CMakeFiles/reduce_optimized_add.dir/reduce_optimized_add.cu.o\u001b[0m\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CUDA executable ../bin/vec_add_opt\u001b[0m\n",
      "[ 66%] Built target vec_add_opt\n",
      "[ 72%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/vec_add_naive.dir/cmake_device_link.o\u001b[0m\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CUDA executable ../bin/vec_add_naive\u001b[0m\n",
      "[ 77%] Built target vec_add_naive\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(131)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"rel_eps\"\u001b[0m was declared but never referenced\n",
      "      float rel_eps = 1e-5f;\n",
      "            ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(132)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"abs_eps\"\u001b[0m was declared but never referenced\n",
      "      float abs_eps = 1e-6f;\n",
      "            ^\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(131)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"rel_eps\"\u001b[0m was declared but never referenced\n",
      "      float rel_eps = 1e-5f;\n",
      "            ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(132)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"abs_eps\"\u001b[0m was declared but never referenced\n",
      "      float abs_eps = 1e-6f;\n",
      "            ^\n",
      "\n",
      "[ 83%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/reduce_optimized_add.dir/cmake_device_link.o\u001b[0m\n",
      "[ 88%] \u001b[32m\u001b[1mLinking CUDA executable ../bin/reduce_optimized_add\u001b[0m\n",
      "/usr/bin/ld: /usr/lib/gcc/x86_64-linux-gnu/11/../../../x86_64-linux-gnu/Scrt1.o: in function `_start':\n",
      "(.text+0x1b): undefined reference to `main'\n",
      "collect2: error: ld returned 1 exit status\n",
      "gmake[2]: *** [reduction/CMakeFiles/reduce_optimized_add.dir/build.make:123: bin/reduce_optimized_add] Error 1\n",
      "gmake[1]: *** [CMakeFiles/Makefile2:321: reduction/CMakeFiles/reduce_optimized_add.dir/all] Error 2\n",
      "gmake[1]: *** Waiting for unfinished jobs....\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(131)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"rel_eps\"\u001b[0m was declared but never referenced\n",
      "      float rel_eps = 1e-5f;\n",
      "            ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(132)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"abs_eps\"\u001b[0m was declared but never referenced\n",
      "      float abs_eps = 1e-6f;\n",
      "            ^\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(131)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"rel_eps\"\u001b[0m was declared but never referenced\n",
      "      float rel_eps = 1e-5f;\n",
      "            ^\n",
      "\n",
      "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "\u001b[01m\u001b[0m\u001b[01m/content/GPU_mode/reduction/reduce_naive_add.cu(132)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"abs_eps\"\u001b[0m was declared but never referenced\n",
      "      float abs_eps = 1e-6f;\n",
      "            ^\n",
      "\n",
      "[ 94%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/reduce_naive_add.dir/cmake_device_link.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CUDA executable ../bin/reduce_naive_add\u001b[0m\n",
      "[100%] Built target reduce_naive_add\n",
      "gmake: *** [Makefile:91: all] Error 2\n"
     ]
    }
   ],
   "source": [
    "# Configure the project (top-level CMakeLists.txt)\n",
    "!cmake -S . -B build -DCMAKE_BUILD_TYPE=Release\n",
    "\n",
    "# Build all targets (matmul, vec_add, etc.)\n",
    "!cmake --build build -j 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe34d054",
   "metadata": {},
   "source": [
    "# Optional build single kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0125bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernels = [\"matmul\", \"vec_add\"]  # target names from each CMakeLists.txt\n",
    "\n",
    "# for k in kernels:\n",
    "#     print(f\"\\n=== Building {k} ===\")\n",
    "#     !cmake --build build --target {k} -j 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df4574",
   "metadata": {},
   "source": [
    "# Running all Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b33cd202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global kernels\n",
    "kernels = [\"reduce_naive_add\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d424f05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running reduce_naive_add ===\n",
      "CPU sum: 1.57286e+06 | GPU sum: 1.57286e+06 | diff: 0\n"
     ]
    }
   ],
   "source": [
    "# kernels = [\"matmul_naive\", \"vec_add_naive\",\"matmul_tiled\",\"vec_add_opt\"]  # add more later: \"softmax\", \"conv\", etc.\n",
    "\n",
    "for k in kernels:\n",
    "    print(f\"\\n=== Running {k} ===\")\n",
    "    !./build/bin/{k}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b565db",
   "metadata": {},
   "source": [
    "# CUDA MemCheck all kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a30afd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2aa92b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t\tmatrix_multiplication  reduction\n",
      "CMakeLists.txt\tprofiles\t       test_colab_server.ipynb\n",
      "include\t\tREADME.md\t       vector_add\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d28f099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !which cuda-memcheck\n",
    "# !which compute-sanitizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2022c2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== compute-sanitizer (memcheck) on reduce_naive_add ===\n",
      "Saved profiles/reduce_naive_add_memcheck.txt\n",
      "========= COMPUTE-SANITIZER\n",
      "CPU sum: 1.57286e+06 | GPU sum: 1.57286e+06 | diff: 0\n",
      "========= ERROR SUMMARY: 0 errors\n",
      "\n",
      "=== compute-sanitizer (racecheck) on reduce_naive_add ===\n",
      "Saved profiles/reduce_naive_add_racecheck.txt\n",
      "========= COMPUTE-SANITIZER\n",
      "CPU sum: 1.57286e+06 | GPU sum: 1.57286e+06 | diff: 0\n",
      "========= RACECHECK SUMMARY: 0 hazards displayed (0 errors, 0 warnings)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"profiles\", exist_ok=True)\n",
    "\n",
    "tools   = [\"memcheck\", \"racecheck\"]\n",
    "# kernels = [\"matmul_naive\", \"vec_add_naive\", \"matmul_tiled\", \"vec_add_opt\"]\n",
    "\n",
    "for k in kernels:\n",
    "    for t in tools:\n",
    "        print(f\"\\n=== compute-sanitizer ({t}) on {k} ===\")\n",
    "        log = f\"profiles/{k}_{t}.txt\"\n",
    "        !compute-sanitizer --tool {t} ./build/bin/{k} > {log} 2>&1\n",
    "        print(f\"Saved {log}\")\n",
    "        !tail -n 20 {log}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3ad80",
   "metadata": {},
   "source": [
    "# NCU Profile each kernel all kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c77d5334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t\tmatrix_multiplication  reduction\n",
      "CMakeLists.txt\tprofiles\t       test_colab_server.ipynb\n",
      "include\t\tREADME.md\t       vector_add\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2a9463e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin\t\tCMakeFiles\t     Makefile\t\t    reduction\n",
      "CMakeCache.txt\tcmake_install.cmake  matrix_multiplication  vector_add\n"
     ]
    }
   ],
   "source": [
    "!ls build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ae20b106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul_naive  matmul_tiled  reduce_naive_add  vec_add_naive  vec_add_opt\n"
     ]
    }
   ],
   "source": [
    "!ls build/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e3bcb975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import IFrame\n",
    "# import os\n",
    "\n",
    "# os.makedirs(\"profiles\", exist_ok=True)\n",
    "\n",
    "# kernels = [\"matmul\", \"vec_add\"]\n",
    "\n",
    "# for k in kernels:\n",
    "#     print(f\"\\n=== Profiling {k} with ncu ===\")\n",
    "#     # NCU_DEFAULTS=\"\" clears any default --export that Colab may set\n",
    "#     !NCU_DEFAULTS=\"\" ncu -f --set full --export html -o profiles/{k} ./build/bin/{k}\n",
    "#     display(IFrame(f\"profiles/{k}.html\", width=1024, height=600))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c12bad58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running Nsight Compute and saving reports ===\n",
      "\n",
      "[1] Profiling reduce_naive_add with ncu ...\n",
      "==PROF== Connected to process 13823 (/content/GPU_mode/build/bin/reduce_naive_add)\n",
      "==PROF== Profiling \"naive_add\" - 0: 0%....50%....100% - 30 passes\n",
      "CPU sum: 1.57286e+06 | GPU sum: 1.57286e+06 | diff: 0\n",
      "==PROF== Disconnected from process 13823\n",
      "==PROF== Report: /content/GPU_mode/profiles/reduce_naive_add.ncu-rep\n",
      "--> Saved report: profiles/reduce_naive_add.ncu-rep\n",
      "--> Duration for reduce_naive_add:\n",
      "    Duration                         ms         3.68\n",
      "\n",
      "All reports saved:\n",
      "total 228K\n",
      "-rw-r--r-- 1 root root  116 Nov 29 03:13 reduce_naive_add_memcheck.txt\n",
      "-rw-r--r-- 1 root root 218K Nov 29 03:13 reduce_naive_add.ncu-rep\n",
      "-rw-r--r-- 1 root root  154 Nov 29 03:13 reduce_naive_add_racecheck.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"profiles\", exist_ok=True)\n",
    "\n",
    "# kernels = [\"matmul_naive\", \"matmul_tiled\",\"vec_add_naive\",\"vec_add_opt\"]\n",
    "\n",
    "# -------- First loop: run profiling & save reports --------\n",
    "print(\"=== Running Nsight Compute and saving reports ===\")\n",
    "for k in kernels:\n",
    "    print(f\"\\n[1] Profiling {k} with ncu ...\")\n",
    "    !NCU_DEFAULTS=\"\" ncu -f --set full -o profiles/{k} ./build/bin/{k}\n",
    "    print(f\"--> Saved report: profiles/{k}.ncu-rep\")\n",
    "    \n",
    "    # Print the kernel Duration from the report\n",
    "    print(f\"--> Duration for {k}:\")\n",
    "    !NCU_DEFAULTS=\"\" ncu --import profiles/{k}.ncu-rep --page details | grep \"Duration\"\n",
    "\n",
    "print(\"\\nAll reports saved:\")\n",
    "!ls -lh profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "65407218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Printing Nsight Compute details for each kernel ===\n",
      "\n",
      "[2] Nsight Compute details page for reduce_naive_add\n",
      "[13823] reduce_naive_add@127.0.0.1\n",
      "  naive_add(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
      "    Section: GPU Speed Of Light Throughput\n",
      "    ----------------------- ----------- ------------\n",
      "    Metric Name             Metric Unit Metric Value\n",
      "    ----------------------- ----------- ------------\n",
      "    DRAM Frequency                  Ghz         5.00\n",
      "    SM Frequency                    Mhz       584.99\n",
      "    Elapsed Cycles                cycle    2,154,740\n",
      "    Memory Throughput                 %         2.51\n",
      "    DRAM Throughput                   %         0.56\n",
      "    Duration                         ms         3.68\n",
      "    L1/TEX Cache Throughput           %         5.02\n",
      "    L2 Cache Throughput               %         2.06\n",
      "    SM Active Cycles              cycle 2,148,697.80\n",
      "    Compute (SM) Throughput           %         1.22\n",
      "    ----------------------- ----------- ------------\n",
      "\n",
      "    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance \n",
      "          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    \n",
      "          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 \n",
      "\n",
      "    Section: GPU Speed Of Light Roofline Chart\n",
      "    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of \n",
      "          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       \n",
      "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      \n",
      "          analysis.                                                                                                     \n",
      "\n",
      "    Section: PM Sampling\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Maximum Buffer Size             Mbyte         1.05\n",
      "    Dropped Samples                sample            0\n",
      "    Maximum Sampling Interval       cycle       20,000\n",
      "    # Pass Groups                                    1\n",
      "    ------------------------- ----------- ------------\n",
      "\n",
      "    Section: Compute Workload Analysis\n",
      "    -------------------- ----------- ------------\n",
      "    Metric Name          Metric Unit Metric Value\n",
      "    -------------------- ----------- ------------\n",
      "    Executed Ipc Active   inst/cycle         0.00\n",
      "    Executed Ipc Elapsed  inst/cycle         0.00\n",
      "    Issue Slots Busy               %         0.12\n",
      "    Issued Ipc Active     inst/cycle         0.00\n",
      "    SM Busy                        %         0.12\n",
      "    -------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 99.94%                                                                                    \n",
      "          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   \n",
      "          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             \n",
      "\n",
      "    Section: Memory Workload Analysis\n",
      "    ----------------- ----------- ------------\n",
      "    Metric Name       Metric Unit Metric Value\n",
      "    ----------------- ----------- ------------\n",
      "    Memory Throughput     Gbyte/s         1.81\n",
      "    Mem Busy                    %         2.51\n",
      "    Max Bandwidth               %         1.37\n",
      "    L1/TEX Hit Rate             %            0\n",
      "    L2 Hit Rate                 %        88.90\n",
      "    Mem Pipes Busy              %         1.22\n",
      "    ----------------- ----------- ------------\n",
      "\n",
      "    Section: Memory Workload Analysis Chart\n",
      "    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  \n",
      "          additional metric could enable the rule to provide more guidance.                                             \n",
      "\n",
      "    Section: Scheduler Statistics\n",
      "    ---------------------------- ----------- ------------\n",
      "    Metric Name                  Metric Unit Metric Value\n",
      "    ---------------------------- ----------- ------------\n",
      "    One or More Eligible                   %         0.12\n",
      "    Issued Warp Per Scheduler                        0.00\n",
      "    No Eligible                            %        99.88\n",
      "    Active Warps Per Scheduler          warp         6.75\n",
      "    Eligible Warps Per Scheduler        warp         0.00\n",
      "    ---------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Local Speedup: 97.49%                                                                                    \n",
      "          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      \n",
      "          issues an instruction every 864.5 cycles. This might leave hardware resources underutilized and may lead to   \n",
      "          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    \n",
      "          6.75 active warps per scheduler, but only an average of 0.00 warps were eligible per cycle. Eligible warps    \n",
      "          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   \n",
      "          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      \n",
      "          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          \n",
      "          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            \n",
      "\n",
      "    Section: Warp State Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Warp Cycles Per Issued Instruction             cycle     5,835.46\n",
      "    Warp Cycles Per Executed Instruction           cycle     5,892.44\n",
      "    Avg. Active Threads Per Warp                                   32\n",
      "    Avg. Not Predicated Off Threads Per Warp                    29.33\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 41.73%                                                                                          \n",
      "          On average, each warp of this kernel spends 2435.1 cycles being stalled after EXIT waiting for all            \n",
      "          outstanding memory operations to complete so that warp's resources can be freed. A high number of stalls due  \n",
      "          to draining warps typically occurs when a lot of data is written to memory towards the end of a kernel. Make  \n",
      "          sure the memory access patterns of these store operations are optimal for the target architecture and         \n",
      "          consider parallelized data reduction, if applicable. This stall type represents about 41.7% of the total      \n",
      "          average of 5835.5 cycles between issuing two instructions.                                                    \n",
      "    ----- --------------------------------------------------------------------------------------------------------------\n",
      "    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         \n",
      "          sampling data. The Kernel Profiling Guide                                                                     \n",
      "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    \n",
      "          on each stall reason.                                                                                         \n",
      "\n",
      "    Section: Instruction Statistics\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Metric Name                              Metric Unit Metric Value\n",
      "    ---------------------------------------- ----------- ------------\n",
      "    Avg. Executed Instructions Per Scheduler        inst     2,457.60\n",
      "    Executed Instructions                           inst      393,216\n",
      "    Avg. Issued Instructions Per Scheduler          inst     2,481.60\n",
      "    Issued Instructions                             inst      397,056\n",
      "    ---------------------------------------- ----------- ------------\n",
      "\n",
      "    Section: Launch Statistics\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Metric Name                          Metric Unit    Metric Value\n",
      "    -------------------------------- --------------- ---------------\n",
      "    Block Size                                                   256\n",
      "    Function Cache Configuration                     CachePreferNone\n",
      "    Grid Size                                                  4,096\n",
      "    Registers Per Thread             register/thread              16\n",
      "    Shared Memory Configuration Size           Kbyte           32.77\n",
      "    Driver Shared Memory Per Block        byte/block               0\n",
      "    Dynamic Shared Memory Per Block       byte/block               0\n",
      "    Static Shared Memory Per Block        byte/block               0\n",
      "    # SMs                                         SM              40\n",
      "    Threads                                   thread       1,048,576\n",
      "    Uses Green Context                                             0\n",
      "    Waves Per SM                                               25.60\n",
      "    -------------------------------- --------------- ---------------\n",
      "\n",
      "    Section: Occupancy\n",
      "    ------------------------------- ----------- ------------\n",
      "    Metric Name                     Metric Unit Metric Value\n",
      "    ------------------------------- ----------- ------------\n",
      "    Block Limit SM                        block           16\n",
      "    Block Limit Registers                 block           16\n",
      "    Block Limit Shared Mem                block           16\n",
      "    Block Limit Warps                     block            4\n",
      "    Theoretical Active Warps per SM        warp           32\n",
      "    Theoretical Occupancy                     %          100\n",
      "    Achieved Occupancy                        %        83.71\n",
      "    Achieved Active Warps Per SM           warp        26.79\n",
      "    ------------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 16.29%                                                                                          \n",
      "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (83.7%) can be the     \n",
      "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
      "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
      "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
      "          optimizing occupancy.                                                                                         \n",
      "\n",
      "    Section: GPU and Memory Workload Distribution\n",
      "    -------------------------- ----------- ------------\n",
      "    Metric Name                Metric Unit Metric Value\n",
      "    -------------------------- ----------- ------------\n",
      "    Average DRAM Active Cycles       cycle      103,889\n",
      "    Total DRAM Elapsed Cycles        cycle  147,214,336\n",
      "    Average L1 Active Cycles         cycle 2,148,697.80\n",
      "    Total L1 Elapsed Cycles          cycle   86,189,800\n",
      "    Average L2 Active Cycles         cycle   218,261.59\n",
      "    Total L2 Elapsed Cycles          cycle  100,775,456\n",
      "    Average SM Active Cycles         cycle 2,148,697.80\n",
      "    Total SM Elapsed Cycles          cycle   86,189,800\n",
      "    Average SMSP Active Cycles       cycle 2,145,400.04\n",
      "    Total SMSP Elapsed Cycles        cycle  344,759,200\n",
      "    -------------------------- ----------- ------------\n",
      "\n",
      "    OPT   Est. Speedup: 6.45%                                                                                           \n",
      "          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    \n",
      "          Maximum instance value is 93.06% above the average, while the minimum instance value is 47.38% below the      \n",
      "          average.                                                                                                      \n",
      "\n",
      "    Section: Source Counters\n",
      "    ------------------------- ----------- ------------\n",
      "    Metric Name               Metric Unit Metric Value\n",
      "    ------------------------- ----------- ------------\n",
      "    Branch Instructions Ratio           %         0.17\n",
      "    Branch Instructions              inst       65,536\n",
      "    Branch Efficiency                   %            0\n",
      "    Avg. Divergent Branches                          0\n",
      "    ------------------------- ----------- ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------- Second loop: re-import reports and show details --------\n",
    "print(\"\\n=== Printing Nsight Compute details for each kernel ===\")\n",
    "for k in kernels:\n",
    "    print(f\"\\n[2] Nsight Compute details page for {k}\")\n",
    "    !NCU_DEFAULTS=\"\" ncu --import profiles/{k}.ncu-rep --page details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "130fb77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t\tmatrix_multiplication  reduction\n",
      "CMakeLists.txt\tprofiles\t       test_colab_server.ipynb\n",
      "include\t\tREADME.md\t       vector_add\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97fd7bb",
   "metadata": {},
   "source": [
    "# Create a Zip File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5a81cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # Adjust this if your repo root is different\n",
    "# repo_root = \"/content/GPU_mode\"\n",
    "# profiles_dir = os.path.join(repo_root, \"profiles\")\n",
    "\n",
    "# # Make sure the profiles folder exists\n",
    "# if os.path.isdir(profiles_dir):\n",
    "#     # Create profiles.zip next to the repo root\n",
    "#     zip_path = os.path.join(repo_root, \"profiles\")\n",
    "#     shutil.make_archive(zip_path, \"zip\", profiles_dir)\n",
    "#     print(f\"Created ZIP: {zip_path}.zip\")\n",
    "# else:\n",
    "#     print(f\"No profiles folder found at: {profiles_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8c331104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ZIP: /content/GPU_mode/profiles_20251129_031351.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "repo_root = \"/content/GPU_mode\"\n",
    "profiles_dir = os.path.join(repo_root, \"profiles\")\n",
    "\n",
    "# Make timestamped name\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "zip_basename = f\"profiles_{timestamp}\"             # without .zip\n",
    "zip_path = os.path.join(repo_root, zip_basename)   # /content/GPU_mode/profiles_YYYYMMDD_HHMMSS\n",
    "\n",
    "if os.path.isdir(profiles_dir):\n",
    "    # Create /content/GPU_mode/profiles_YYYYMMDD_HHMMSS.zip\n",
    "    shutil.make_archive(zip_path, \"zip\", profiles_dir)\n",
    "    zip_file = zip_path + \".zip\"\n",
    "    print(f\"Created ZIP: {zip_file}\")\n",
    "else:\n",
    "    print(f\"No profiles folder found at: {profiles_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "18d3a14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t\t       profiles\t\t\t     test_colab_server.ipynb\n",
      "CMakeLists.txt\t       profiles_20251129_031351.zip  vector_add\n",
      "include\t\t       README.md\n",
      "matrix_multiplication  reduction\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a8dbf",
   "metadata": {},
   "source": [
    "# File Downloader Widget/Workflow Provided by Colab For the Profiler Zip File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7fa9706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anywidget\n",
    "import traitlets\n",
    "import os\n",
    "import base64\n",
    "import mimetypes\n",
    "\n",
    "class FileDownloader(anywidget.AnyWidget):\n",
    "    \"\"\"\n",
    "    An anywidget that renders a button. When clicked, it triggers a server-side\n",
    "    read of 'file_path' and sends the content to the browser for download.\n",
    "\n",
    "    The button starts disabled and only enables after a successful handshake\n",
    "    with the Python kernel, ensuring it doesn't appear active in a dead notebook.\n",
    "    \"\"\"\n",
    "\n",
    "    # The path to the file on the server/local disk that you want to download\n",
    "    file_path = traitlets.Unicode(help=\"Path to the file to be downloaded\").tag(sync=True)\n",
    "\n",
    "    # Label for the button\n",
    "    button_text = traitlets.Unicode(\"Download File\").tag(sync=True)\n",
    "\n",
    "    _esm = \"\"\"\n",
    "    export function render({ model, el }) {\n",
    "      // Create the button element\n",
    "      let btn = document.createElement(\"button\");\n",
    "      btn.classList.add(\"jupyter-widgets\", \"jupyter-button\", \"widget-button\");\n",
    "      btn.style.width = \"100%\";\n",
    "\n",
    "      // Initial state: Disabled and waiting\n",
    "      btn.innerText = \"Waiting for Kernel...\";\n",
    "      btn.disabled = true;\n",
    "\n",
    "      // Update button text if the Python trait changes\n",
    "      model.on(\"change:button_text\", () => {\n",
    "        // Only update visually if we are already connected/enabled\n",
    "        if (!btn.disabled) {\n",
    "            btn.innerText = model.get(\"button_text\");\n",
    "        }\n",
    "      });\n",
    "\n",
    "      // Handle the click event\n",
    "      btn.addEventListener(\"click\", () => {\n",
    "        const filePath = model.get(\"file_path\");\n",
    "\n",
    "        if (!filePath) {\n",
    "            alert(\"No file path set in the Python widget!\");\n",
    "            return;\n",
    "        }\n",
    "\n",
    "        // Disable button and show loading state\n",
    "        const originalText = btn.innerText;\n",
    "        btn.innerText = \"Downloading...\";\n",
    "        btn.disabled = true;\n",
    "\n",
    "        // Send a request message to the Python backend\n",
    "        model.send({ type: \"request_download\" });\n",
    "\n",
    "        // Helper to restore button state\n",
    "        const restoreBtn = () => {\n",
    "            btn.innerText = originalText;\n",
    "            btn.disabled = false;\n",
    "        };\n",
    "\n",
    "        // Timeout safety to restore button if Python doesn't respond within 5s\n",
    "        setTimeout(restoreBtn, 5000);\n",
    "      });\n",
    "\n",
    "      el.appendChild(btn);\n",
    "\n",
    "      // Listen for messages coming from Python\n",
    "      model.on(\"msg:custom\", (msg) => {\n",
    "        if (msg.type === \"connection_verified\") {\n",
    "            // HANDSHAKE COMPLETE: Kernel is alive.\n",
    "            btn.disabled = false;\n",
    "            btn.innerText = model.get(\"button_text\");\n",
    "        }\n",
    "        else if (msg.type === \"file_content\") {\n",
    "            // 1. Create a Blob from the Base64 data\n",
    "            const byteCharacters = atob(msg.content);\n",
    "            const byteNumbers = new Array(byteCharacters.length);\n",
    "            for (let i = 0; i < byteCharacters.length; i++) {\n",
    "                byteNumbers[i] = byteCharacters.charCodeAt(i);\n",
    "            }\n",
    "            const byteArray = new Uint8Array(byteNumbers);\n",
    "            const blob = new Blob([byteArray], { type: msg.mime_type });\n",
    "\n",
    "            // 2. Create a temporary link to trigger the download\n",
    "            const url = window.URL.createObjectURL(blob);\n",
    "            const a = document.createElement(\"a\");\n",
    "            a.style.display = \"none\";\n",
    "            a.href = url;\n",
    "            a.download = msg.filename;\n",
    "            document.body.appendChild(a);\n",
    "            a.click();\n",
    "\n",
    "            // 3. Cleanup\n",
    "            window.URL.revokeObjectURL(url);\n",
    "            document.body.removeChild(a);\n",
    "\n",
    "            // Restore button text\n",
    "            btn.innerText = model.get(\"button_text\");\n",
    "            btn.disabled = false;\n",
    "\n",
    "        } else if (msg.type === \"error\") {\n",
    "            alert(`Error: ${msg.message}`);\n",
    "            btn.innerText = model.get(\"button_text\");\n",
    "            btn.disabled = false;\n",
    "        }\n",
    "      });\n",
    "\n",
    "      // INITIATE HANDSHAKE\n",
    "      // Send a message to Python to check if the kernel is listening.\n",
    "      // If the kernel is dead (saved notebook), this message goes nowhere,\n",
    "      // and the button remains disabled.\n",
    "      setTimeout(() => {\n",
    "        model.send({ type: \"check_connection\" });\n",
    "      }, 500);\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if file_path:\n",
    "            self.file_path = file_path\n",
    "\n",
    "        # Register the message handler\n",
    "        self.on_msg(self._handle_custom_msg)\n",
    "\n",
    "    def _handle_custom_msg(self, msg, content):\n",
    "        \"\"\"\n",
    "        Callback for when the frontend sends a message to Python.\n",
    "        \"\"\"\n",
    "        msg_type = msg.get(\"type\")\n",
    "\n",
    "        if msg_type == \"check_connection\":\n",
    "            # Reply to the frontend to confirm we are alive\n",
    "            self.send({\"type\": \"connection_verified\"})\n",
    "\n",
    "        elif msg_type == \"request_download\":\n",
    "            self._process_download()\n",
    "\n",
    "    def _process_download(self):\n",
    "        \"\"\"\n",
    "        Reads the file from disk and sends it to the frontend.\n",
    "        \"\"\"\n",
    "        target_path = self.file_path\n",
    "\n",
    "        # Basic validation\n",
    "        if not target_path:\n",
    "            self.send({\"type\": \"error\", \"message\": \"File path is not defined.\"})\n",
    "            return\n",
    "\n",
    "        if not os.path.exists(target_path):\n",
    "            self.send({\"type\": \"error\", \"message\": f\"File not found: {target_path}\"})\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Guess the MIME type so the browser handles it correctly\n",
    "            mime_type, _ = mimetypes.guess_type(target_path)\n",
    "            if mime_type is None:\n",
    "                mime_type = 'application/octet-stream'\n",
    "\n",
    "            # Read and encode the file\n",
    "            with open(target_path, \"rb\") as f:\n",
    "                file_content = f.read()\n",
    "\n",
    "            b64_content = base64.b64encode(file_content).decode(\"utf-8\")\n",
    "\n",
    "            # Send back to JS\n",
    "            self.send({\n",
    "                \"type\": \"file_content\",\n",
    "                \"filename\": os.path.basename(target_path),\n",
    "                \"mime_type\": mime_type,\n",
    "                \"content\": b64_content\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            self.send({\"type\": \"error\", \"message\": str(e)})\n",
    "\n",
    "# dummy_filename = \"example_data.txt\"\n",
    "# with open(dummy_filename, \"w\") as f:\n",
    "#     f.write(\"Hello! This is a file dynamically read from the kernel disk.\\n\")\n",
    "#     f.write(\"If you are reading this, the widget worked.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9d35434b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/GPU_mode/profiles_20251129_031351.zip\n"
     ]
    }
   ],
   "source": [
    "print(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7fcb143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t\t       profiles\t\t\t     test_colab_server.ipynb\n",
      "CMakeLists.txt\t       profiles_20251129_031351.zip  vector_add\n",
      "include\t\t       README.md\n",
      "matrix_multiplication  reduction\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81de5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27e74e2caa74217bb166e2fe6cfe77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "<__main__.FileDownloader object at 0x7ed2003abef0>"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the timestamped zip with FileDownloader\n",
    "# FileDownloader(file_path=\"/content/GPU_mode/test_colab_server.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135d312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/content/GPU_mode/profiles_20251129_031351.zip' target='_blank'>/content/GPU_mode/profiles_20251129_031351.zip</a><br>"
      ],
      "text/plain": [
       "/content/GPU_mode/profiles_20251129_031351.zip"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from IPython.display import FileLink\n",
    "\n",
    "# zip_path = \"/content/GPU_mode/profiles_run_2025-11-28T19-12-00.zip\"\n",
    "# FileLink(zip_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8963a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

// #include <iostream>
// #include <math.h>
// #include <numeric>
// #include <vector>
// #include <assert.h>
// #include "cuda_utils.hpp"

// #define BLOCK_SZ 256


// __global__ void naive_add(float *input, float output, int N){
//     int idx = blockDim.x * blockIdx.x + threadIdx.x;
    
//     if(idx < N)
//         printf("Generated by thread %d , %f + %f = %f(New Output)",
//                                     idx,input,output,atomicAdd(output,input[idx]));
// }


// int main(){
//     // get sizes properly
//     const int N = 1 << 20;  // 1M elements
//     const size_t bytes = N * sizeof(float);

//     // give proper C++ STL containers for CUDA work
//     std::vector<float> h_a(N);

//     // either populate them or get input from them
//     // always pass flat contigous arrays to CUDA Kernels
//     for (int i = 0; i < N; ++i) {
//         if(i % 2 == 0)
//             h_a[i] = 1.0f;
//         else
//             h_a[i] = 2.0f;
//     }

//     // ground truth ,test later 
//     float sum_truth = std::accumulate(h_a.begin(),h_a.end(),0);

//     // device variables
//     float *d_input, d_output;
    
//     // Allocate memory on device
//     CUDA_CHECK(cudaMalloc(d_input,bytes));
//     CUDA_CHECK(cudaMalloc(d_input,bytes));

//     // copy host variable's data to appropriate device variables or use cudaMemSet for this
//     CUDA_CHECK(cudaMemcpy(d_input,h_input.data(),bytes,cudaMemcpyHostToDevice));

//     // memset output to a particular default value
//     CUDA_CHECK(cudaMemset(d_output,0.0f,bytes));

//     dim3 blockSize = BLOCK_SZ;
//     dim3 gridSize = (N + blockSize  - 1) / blockSize;

//     naive_add<<<gridSize,blockSize>>>(d_input,d_output,N);


//     assert(d_output == sum_truth);
//     std::cout << "The output is" << d_output;
//     cudaFree(d_input);
//     cudaFree(d_output);

//     return 0;
// }


#include <iostream>
#include <vector>
#include <numeric>
#include <cmath>
#include <cassert>
#include "cuda_utils.hpp" 

#define BLOCK_SZ 256

// Naive "reduction" using atomicAdd into a single global float
__global__ void naive_add(const float* __restrict__ input,
                          float* __restrict__ output,
                          int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        // Output to single float in device Memory
        atomicAdd(output, input[idx]);  
    }
}

int main() {
    // Problem size
    const int N = 1 << 20;  // 1M elements
    const size_t bytes = N * sizeof(float);

    // Host data
    std::vector<float> h_a(N);
    for (int i = 0; i < N; ++i) {
        h_a[i] = (i % 2 == 0) ? 1.0f : 2.0f;  
    }

    // CPU ground truth
    float sum_truth = std::accumulate(h_a.begin(), h_a.end(), 0.0f);

    // Device pointers
    float* d_input  = nullptr;
    float* d_output = nullptr;

    // Allocate device memory
    CUDA_CHECK(cudaMalloc(&d_input,  bytes));         
    CUDA_CHECK(cudaMalloc(&d_output, sizeof(float)));

    // Copy input to device
    CUDA_CHECK(cudaMemcpy(d_input, h_a.data(), bytes, cudaMemcpyHostToDevice));

    // Initialize accumulator to 0.0f
    CUDA_CHECK(cudaMemset(d_output, 0, sizeof(float)));

    // Kernel launch configuration
    dim3 blockSize(BLOCK_SZ);
    dim3 gridSize((N + blockSize.x - 1) / blockSize.x);

    // Launch kernel
    naive_add<<<gridSize, blockSize>>>(d_input, d_output, N);
    CUDA_CHECK(cudaGetLastError());
    CUDA_CHECK(cudaDeviceSynchronize());

    // Copy result back
    float h_sum = 0.0f;
    CUDA_CHECK(cudaMemcpy(&h_sum, d_output, sizeof(float), cudaMemcpyDeviceToHost));

    // Compare with tolerance (approx for floats)
    float diff   = std::fabs(h_sum - sum_truth);
    float max_ab = std::max(std::fabs(h_sum), std::fabs(sum_truth));
    float rel_eps = 1e-5f;
    float abs_eps = 1e-6f;

    // test changes
    assert(diff <= std::max(abs_eps, rel_eps * max_ab));

    std::cout << "CPU sum: " << sum_truth
              << " | GPU sum: " << h_sum
              << " | diff: " << diff << std::endl;

    // Cleanup
    cudaFree(d_input);
    cudaFree(d_output);

    return 0;
}
